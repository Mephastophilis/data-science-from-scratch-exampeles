{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 17 - Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision trees can easily handle a mix of numer and categorical attributes and can even classify data for which attributes are missing. Easy to understand and interpret, and the process of reaching a prediction is transparent.\n",
    "\n",
    "However, finding the optimal decision tree for a training set is a computationally hard problem. It's easy to build a decision tree that is overfitted to the training data, which generalize poorly to unseen data.\n",
    "\n",
    "Decision trees can be divided into classification trees and regression trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entropy: uncertainty associated with the data. Imagine a set $S$ of data, each member of which is labeled and belongs to a set of finite classes $C_1, ..., C_n$. If all data belongs to a single class, there is no uncertainty and low entropy. If the data is spread out evenly amongst all classes, then there is high entropy. In math terms, $p_i$ is the proportion of data labeld as class $c_i$. Then etropy is: <br>\n",
    "\n",
    "<center> $H(S) = - p_1 log_2 p_1 - ... - p_n log_2 p_n$, where 0 log 0 = 0. </center>\n",
    "$-p_i log_2 p_i$ is non-negative and close to zero precisely when p_i is either close to zero or close to one. So entropy is small when every $p_i$ is close to 0 or 1, and large when $p_i$'s are not close to 1 or 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(class_probabilities):\n",
    "    \"\"\"given a list of class probabilities, compute the entropy\"\"\"\n",
    "    return sum(-p * math.log(p,2) for p in class_probabilities if p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_probabilites(labels):\n",
    "    total_count = len(labels)\n",
    "    return [count / total_count for count in Counter(labels).values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_entropy(labeled_data):\n",
    "    labels = [label for _, label in labeled_data]\n",
    "    probabilities = class_probabilites(labels)\n",
    "    return entropy(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_entropy(subsets):\n",
    "    \"\"\"find the entropy from this partition of data into subsets\n",
    "    sub sets is a list of lists of labeled data\"\"\"\n",
    "    \n",
    "    total_count = sum(len(subset) for subset in subsets)\n",
    "    \n",
    "    return sum( data_entropy(subset) * len(subset) / total_count for subset in subsets )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [\n",
    "        ({'level':'Senior','lang':'Java','tweets':'no','phd':'no'},   False),\n",
    "        ({'level':'Senior','lang':'Java','tweets':'no','phd':'yes'},  False),\n",
    "        ({'level':'Mid','lang':'Python','tweets':'no','phd':'no'},     True),\n",
    "        ({'level':'Junior','lang':'Python','tweets':'no','phd':'no'},  True),\n",
    "        ({'level':'Junior','lang':'R','tweets':'yes','phd':'no'},      True),\n",
    "        ({'level':'Junior','lang':'R','tweets':'yes','phd':'yes'},    False),\n",
    "        ({'level':'Mid','lang':'R','tweets':'yes','phd':'yes'},        True),\n",
    "        ({'level':'Senior','lang':'Python','tweets':'no','phd':'no'}, False),\n",
    "        ({'level':'Senior','lang':'R','tweets':'yes','phd':'no'},      True),\n",
    "        ({'level':'Junior','lang':'Python','tweets':'yes','phd':'no'}, True),\n",
    "        ({'level':'Senior','lang':'Python','tweets':'yes','phd':'yes'},True),\n",
    "        ({'level':'Mid','lang':'Python','tweets':'no','phd':'yes'},    True),\n",
    "        ({'level':'Mid','lang':'Java','tweets':'yes','phd':'no'},      True),\n",
    "        ({'level':'Junior','lang':'Python','tweets':'no','phd':'yes'},False)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_by(inputs, attribute):\n",
    "    \"\"\"each input is a pair (attribute_dict, label)\n",
    "    returns a dict : attribute_value -> inputs\"\"\"\n",
    "    \n",
    "    groups = defaultdict(list)\n",
    "    for input in inputs:\n",
    "        key = input[0][attribute]\n",
    "        groups[key].append(input)\n",
    "    return groups    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_entropy_by(inputs, attribute):\n",
    "    \"\"\"computes the entropy corresponding to the give partition\"\"\"\n",
    "    partitions = partition_by(inputs, attribute)\n",
    "    return partition_entropy(partitions.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level 0.6935361388961919\n",
      "lang 0.8601317128547441\n",
      "tweets 0.7884504573082896\n",
      "phd 0.8921589282623617\n"
     ]
    }
   ],
   "source": [
    "for key in ['level', 'lang', 'tweets', 'phd']:\n",
    "    print(key, partition_entropy_by(inputs, key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "senior_inputs = [(input, label) for input, label in inputs if input[\"level\"] == \"Senior\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lang 0.4\n",
      "tweets 0.0\n",
      "phd 0.9509775004326938\n"
     ]
    }
   ],
   "source": [
    "for key in ['lang', 'tweets', 'phd']:\n",
    "    print(key, partition_entropy_by(senior_inputs, key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mid_inputs = [(input, label) for input, label in inputs if input[\"level\"] == \"Mid\"]\n",
    "\n",
    "junior_inputs = [(input, label) for input, label in inputs if input[\"level\"] == \"Junior\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lang 0.0\n",
      "tweets 0.0\n",
      "phd 0.0\n"
     ]
    }
   ],
   "source": [
    "for key in ['lang', 'tweets', 'phd']:\n",
    "    print(key, partition_entropy_by(mid_inputs, key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lang 0.9509775004326938\n",
      "tweets 0.9509775004326938\n",
      "phd 0.0\n"
     ]
    }
   ],
   "source": [
    "for key in ['lang', 'tweets', 'phd']:\n",
    "    print(key, partition_entropy_by(junior_inputs, key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "junior_phd_inputs = [(input, label) for input, label in junior_inputs if input[\"phd\"] == 'yes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lang 0.0\n",
      "tweets 0.0\n"
     ]
    }
   ],
   "source": [
    "for key in ['lang', 'tweets']:\n",
    "    print(key, partition_entropy_by(junior_phd_inputs, key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a tree to be one of the following:\n",
    "* True\n",
    "* False\n",
    "* a tuple (attribute, subtree_dict)\n",
    "\n",
    "Here True represents a leaf node that returns True for any input False represents a leaf node that returns False for any input, and a tuple represents a decision node that, for any input, finds its attribute value, and classifies the input using the corresponding subtree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(tree, input):\n",
    "    \"\"\"classify the input using the given decision tree\"\"\"\n",
    "    \n",
    "    #if this is a leaf node, return its value\n",
    "    if tree in [True, False]:\n",
    "        return tree\n",
    "    \n",
    "    #otherwise this tree consists of an attribute to split on and a dictionary whose keys are values of that atrribute\n",
    "    # and whose values of are subtrees to consider next attribute, subtree_dict = tree\n",
    "    attribute, subtree_dict = tree\n",
    "    \n",
    "    subtree_key = input.get(attribute)   #None if input is missing\n",
    "    \n",
    "    if subtree_key not in subtree_dict:   # if no subtree key,\n",
    "        subtree_key = None                # we'll use the None subtree\n",
    "    \n",
    "    subtree = subtree_dict[subtree_key]\n",
    "    return classify(subtree, input)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree_id3(inputs, split_candidates=None):\n",
    "    \n",
    "    #if this is our first pass, all keys of the first input are split candidates\n",
    "    \n",
    "    if split_candidates is None:\n",
    "        split_candidates = inputs[0][0].keys()\n",
    "        \n",
    "    # count Trues and Falses in the inputs\n",
    "    num_inputs = len(inputs)\n",
    "    num_trues = len([label for item, label in inputs if label])\n",
    "    num_falses = num_inputs - num_trues\n",
    "    \n",
    "    if num_trues == 0: return False # no trues, return \"False\" leaf\n",
    "    if num_falses == 0: return True # no fales, return \"True\" leaf\n",
    "    \n",
    "    if not split_candidates:\n",
    "        return num_trues >= num_falses\n",
    "    \n",
    "    #otherwise split on the best attribute\n",
    "    best_attribute = min(split_candidates, key=partial(partition_entropy_by, inputs))\n",
    "    \n",
    "    partitions = partition_by(inputs, best_attribute)\n",
    "    new_candidates = [a for a in split_candidates if a != best_attribute]\n",
    "    \n",
    "    # recursively build the subtrees\n",
    "    subtrees = {attribute_value : build_tree_id3(subset, new_candidates) \n",
    "                for attribute_value, subset in partitions.items() }\n",
    "    \n",
    "    subtrees[None] = num_trues > num_falses   # default case\n",
    "    \n",
    "    return (best_attribute, subtrees)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = build_tree_id3(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Junior / Java / tweets / no phd True\n"
     ]
    }
   ],
   "source": [
    "print(\"Junior / Java / tweets / no phd\", classify(tree, \n",
    "        { \"level\" : \"Junior\", \n",
    "          \"lang\" : \"Java\", \n",
    "          \"tweets\" : \"yes\", \n",
    "          \"phd\" : \"no\"} ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Junior / Java / tweets / phd False\n"
     ]
    }
   ],
   "source": [
    "print(\"Junior / Java / tweets / phd\", classify(tree, \n",
    "        { \"level\" : \"Junior\", \n",
    "                 \"lang\" : \"Java\", \n",
    "                 \"tweets\" : \"yes\", \n",
    "                 \"phd\" : \"yes\"} ) )\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intern True\n"
     ]
    }
   ],
   "source": [
    "print(\"Intern\", classify(tree, { \"level\" : \"Intern\" } ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Senior False\n"
     ]
    }
   ],
   "source": [
    "print(\"Senior\", classify(tree, { \"level\" : \"Senior\" } ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forest_classify(trees, input):\n",
    "    votes = [classify(tree, input) for tree in trees]\n",
    "    vote_counts = Counter(votes)\n",
    "    return vote_counts.most_common(1)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduce randomness by fitting trees to random subsets of the data (bootstrapping) and then test the trees on the remaining data points. Choose the tree in the forest with the best fit on the test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also introduce randomness by changing the way we chose the best_attribute to split on. Rather than looking at all the remaining attributes, we first choose a random subset of them and then split on whichever of those is best:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example function:\n",
    "\n",
    "def random_attribute_selection():\n",
    "    \n",
    "    if len(split_candidates) <= self.num_split_candidates:\n",
    "        sampled_split_candidates = split_candidates\n",
    "        \n",
    "    else:\n",
    "        sampled_split_candidates = random.smaple(split_candidates, self.num_split_candidates)\n",
    "        \n",
    "    best_attribute = min(sampled_split_candidates, key = partial(partition_entropy_by, inputs))\n",
    "    \n",
    "    partitions = partitions_by(inputs, best_attribute)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
